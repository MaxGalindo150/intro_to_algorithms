{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "Linear regression is used when we want to predict a numerical value. Common exaples include predicting prices (of homes, stocks, etc.). However, not every prediction problem involves classical regression. For instance, in classification problems, the goal is to predict membership in a set of categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics\n",
    "Linear regression flows frim a few simple assumptions:\n",
    "- 1.- We assume that the relationship between features **$\\mathbf{x}$** and target $y$ is approximately linear, i.e., that the conditional mean $E[Y | X = \\mathbf{x}]$ can be expressed as a weighted sum of the features $\\mathbf{x}$.\n",
    "- 2.- We can impose the assumption that any such noise is well behaved, following a Gaussion distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "The model is responsable to describe how features can be transformed into an estimate of the target. The assumption of linearity means thath the expected value of the target (price) can be expressed as a weighted sum of the features (area and age):\n",
    "\n",
    "$$ price = w_{area} \\cdot area + w_{age} \\cdot age + b.$$\n",
    "\n",
    "Here $w_{area}$ and $w_{age}$ are called weights, and $b$ is called *bias* (or *offset* or *intercept*). The wieghts determine the influence of each feature on our prediction. The bias determines the value of the estimate when all features are zero, we still need the bias beacuase it allows us to express all linear function of our features (rather than restricting us to lines that pass through the origin). \n",
    "\n",
    "Strictly speaking, is an *affine transformation* of input features, which is characterized by a *linear transformation* of features via a weighted sum, combined with a *translation* via the added bias. Given a dataset, our goal is to choose the weights $\\mathbf{w}$ and the bias $y$ that, on average, make our model's predictions fit the true prices observed in the data as closely as possible.\n",
    "\n",
    "To work with high dimensional dataset we adopt the following notation:\n",
    "\n",
    "$$\\hat{y} = w_1 x_1 + \\dots + w_d x_d + b$$\n",
    "\n",
    "where the symbol \"hat\" denotes an estimate. We can collect the features into a vector $\\mathbf{x} \\in \\mathbb{R}^d$ and all the wieghts into a vector $\\mathbf{w} \\in \\mathbb{R}^d$, therefore:\n",
    "\n",
    "$$\\hat{y} = \\mathbf{w}^{T}\\mathbf{x} + b$$\n",
    "\n",
    "the vector $\\mathbf{x}$ corresponds to the features of a single example. We will often find it convenient to refer to features of our entire dataset of $n$ examples via the *design matrix* $\\mathbf{X} \\in \\mathbb{R}^{n\\times d}$, therefore:\n",
    "\n",
    "$$\\mathbf{\\hat{y}} = \\mathbf{X}\\mathbf{w} + b$$\n",
    "\n",
    "Before we can got about searching for the best *parameters* (or *model parameters*) $\\mathbf{w}$ and $b$, we will need two more things: (i) a measure of the quality of some given model; and a procedure for updating the model to imporve its quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "The *loss function* quantify the distance between the *real* and *predictive* values of the target. The loss will usually be a nonnegative number ehere samaller values are better an perfect predictions incur a loss of $0$. For regression prblems, the most common loss function is the squared error. When our prediction for an example $i$ is $\\hat{y}^{(i)}$ and the corresponding true label is $\\hat{y}^{(i)}$, the *squared error* is given by:\n",
    "\n",
    "$$\n",
    "l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2}\\left(\\hat{y}^{(i)} - y^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "Note that large differences between estimates $\\hat{y}^{(i)}$ and tagets $y^{(i)}$ lead to even larger contributions to the loss, due to its quadratic form (this quadraticity can be a bouble-edge sword; while it encourages the model to avoid large errors it can also lead to excessive sensitivity to anomalous data). To measure the quality of a model on the enire dataset of $n$ examples, we simpy avarege (or equivalently, sum) the losses on the training set:\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w}, b) = \\frac{1}{n}\\sum_{i=1}^{n}l^{(i)}(\\mathbf{w}, b) = \\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{2}\\left( \\mathbf{w}^{T}\\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2\n",
    "$$\n",
    "\n",
    "\n",
    "When training the model, we seek parameters ($\\mathbf{w}^*$, $b^*$) that minimize the total loss across all training examples:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^*, b^* = \\argmin_{\\substack{\\mathbf{w}, b}} L(\\mathbf{w}, b)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
